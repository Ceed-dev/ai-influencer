# AI-Influencer - Context

This file maintains the complete project history for AI continuity.

## Project Purpose

Manage the complete AI influencer video production lifecycle: component management, video production planning, multi-platform analytics (YouTube Shorts / TikTok / Instagram Reels), and AI-driven improvement recommendations.

## Key IDs and Resources

| Resource | ID / URL |
|----------|----------|
| Master Spreadsheet | `1fI1s_KLcegpiACJYpmpNe9tnQmnZo2o8eHIXNV5SpPg` |
| Apps Script Project | `1nbGkujRb6PwtkVuxBh1Vb9NKIUFi_uBa8NbdPSEx_PoxREmyMUKivA0S` |
| Root Drive Folder (AI-Influencer) | `1KRQuZ4W7u5CXRamjvN4xmavfu-7TPb0X` |
| Web App Deployment URL | `https://script.google.com/macros/s/{DEPLOYMENT_ID}/exec` (created via `clasp deploy`) |
| GitHub Repository | `Ceed-dev/ai-influencer` (旧: `video-analytics-hub`) |

### Inventory Spreadsheet IDs (stored in Script Properties)

These are auto-generated by `setupCompleteSystem()` and stored in GAS Script Properties:
- `SCENARIOS_INVENTORY_ID`
- `MOTIONS_INVENTORY_ID`
- `CHARACTERS_INVENTORY_ID`
- `AUDIO_INVENTORY_ID`

---

## Session History

### 2026-02-06: Project Initialization (v1.0)

**What was done:**
- Created project folder structure
- Created initial documentation (README.md, ARCHITECTURE.md, CONTEXT.md)
- Defined GAS project structure with component breakdown
- Established Google Sheets schema for multi-platform metrics

**Key decisions:**
1. **GAS over GCP VM**: Chose Google Apps Script for simplicity and native Sheets integration
2. **video_uid linking**: Each video gets a unique ID to link across platforms
3. **CSV-first approach**: Platform APIs have severe limitations. CSV export is the primary data source

### 2026-02-06: Phase 2-5 Implementation (v1.0)

**What was done:**
- CSV parsers, normalizers, linkers, KPI engine, LLM analyzer, sheet writer
- 223 tests across 6 test suites
- n8n integration documentation
- Sample CSV files

### 2026-02-09: Complete v2.0 Rebuild

**Why v2.0 was needed:**
- v1.0 was analytics-only (CSV import and analysis). It had no concept of video production lifecycle, no component management, and no way to track which content elements contributed to performance.
- v2.0 adds the complete production loop: component inventories, production workflow, approval gates, component-aware AI analysis, and performance scoring.

**What was done:**

- **Phase 1: Foundation**
  - `Config.gs` - Complete rewrite with Drive structure, master columns, component types, ID prefixes, KPI defaults, column aliases, dropdown options, colors
  - `Utils.gs` - Added readSheetAsObjects, findRowByColumn, findAllRowsByColumn, updateRowByIndex, ID generators (generateVideoUid, generateComponentId, generateScenarioId, generateMotionId, generateCharacterId, generateAudioId), getInventoryTypeFromId, getComponentById, getComponentsById
  - `Setup.gs` - One-click system setup: creates Drive folders (Scenarios/Motions/Characters/Audio/Analytics with subfolders), inventory spreadsheets (4 separate spreadsheets), all master sheet tabs, demo data, KPI defaults, formatting (dropdowns, conditional colors, checkbox)
  - `Migration.gs` - v1 to v2 migration (renames videos_master to master, adds new columns, creates video_analysis sheet, updates recommendations schema)

- **Phase 2: Data Layer**
  - `ComponentManager.gs` (NEW) - Component CRUD (addComponent, updateComponent, archiveComponent), listComponents with filters (type/status/tags), getTopPerformingComponents, buildVideoComponentContext, incrementComponentUsage, getComponentPerformanceHistory, buildRecommendationComponentPool
  - `MasterManager.gs` (NEW) - Master sheet operations (createProduction, getVideosByStatus, getApprovedVideos, getProductionData), production workflow (updateVideoStatus, approveVideo), updateMetricsSnapshot, writeAIRecommendations, updateAnalysisResults, getAllVideoUids, getMasterData
  - `ScoreUpdater.gs` (NEW) - updateAllComponentScores, calculateAvgScore, updateComponentScoresForVideo, updateSingleComponentScore, normalizeOverallScore, getScoreSummary

- **Phase 3: Analysis Extension**
  - `LLMAnalyzer.gs` - Rewritten with component-aware prompts (analyzWithLLMEnhanced), includes component history and top performers in AI context, generates component-specific recommendations via separate prompt, single-video analysis with component context
  - `Linker.gs` - Updated for v2.0 master sheet references, added updateMasterMetricsSnapshot
  - `KPIEngine.gs` - Added normalizeOverallScore for cross-platform scoring
  - `SheetWriter.gs` - Added writeToInventory, writeVideoAnalysis, writeRecommendationsEnhanced, clearAllData

- **Phase 4: Integration**
  - `Code.gs` - Complete rewrite with 13 API actions (doGet: get_status, get_approved, get_production, get_components, get_score_summary; doPost: import_csv, analyze, analyze_single, analyze_all, link_videos, create_production, approve_video, update_status, add_component, update_component, get_components, update_scores)
  - UI menu with submenus: Import CSV (YouTube/TikTok/Instagram), Analyze (Single/All), Production (Create/Approve/Status), Components (Add/Browse/Scores)

- **Phase 5: Testing**
  - 3 new test files: ComponentManager.test.js, MasterManager.test.js, ScoreUpdater.test.js
  - Updated all existing tests for v2.0 (master sheet references, removed scenario_cuts)
  - Updated setup.js with all v2.0 utility mocks
  - **330 tests passing across 9 test suites**

- **Phase 6: Documentation**
  - Updated all documentation for v2.0

**Key v2.0 decisions:**
1. **Separate inventory spreadsheets**: Each component type (scenarios, motions, characters, audio) has its own spreadsheet, accessed via `SpreadsheetApp.openById()`
2. **Component ID system**: Prefix-based (SCN_H_, SCN_B_, SCN_C_, MOT_, CHR_, AUD_) for type identification
3. **Production workflow**: draft -> approved -> in_production -> published -> analyzed, with human approval gate
4. **Component-aware AI analysis**: AI receives full component context (history, scores, top performers) when analyzing

### 2026-02-09: v2.0.1 - Checkbox Bug Fix

**Problem:** The `human_approved` checkbox column in the master sheet was causing `getLastRow()` to return 101 instead of 2 (or the actual data row count), because `requireCheckbox()` data validation auto-fills `FALSE` into empty cells, making them appear as data rows.

**Fix:** Changed `applyMasterSheetFormatting_()` to only apply checkbox validation to rows that have actual data (checked `sheet.getLastRow()` before applying), rather than applying to a range of 100 rows.

**Commit:** `62f7400 Fix insertDemoData false positive: checkbox FALSE values triggered data check`

### 2026-02-09: v2.0.2 - appendRow Bug Fix

**Problem:** Because of the checkbox FALSE values filling up to row 101, `sheet.appendRow()` was writing new data at row 102 instead of the next available row after actual data.

**Fix:** Updated `insertDemoData()` to clear any existing rows (including checkbox FALSE values) before writing demo data, and then apply checkbox validation only to the data rows.

**Commit:** `4c36712 Fix checkbox causing appendRow to write at row 101 instead of row 2`

### 2026-02-09: v2.0.3 - Config Sheet Fallback for API Keys

**Problem:** Setting Script Properties via the Apps Script editor is cumbersome, especially for the OpenAI API key. The `_config` sheet in the spreadsheet provides a more accessible way to store configuration.

**What changed:**
- `Config.gs`: Added `getConfigFromSheet_()` function that reads from a `_config` sheet (columns: key, value, description) as a fallback when Script Properties are not set
- `Config.gs`: Added `migrateConfigToProperties()` to migrate _config sheet values to Script Properties
- `OPENAI_API_KEY` now reads from Script Properties first, then falls back to `_config` sheet

### 2026-02-09: v2.0.4 - Web App Access Configuration

**Problem:** The Web App was not accessible externally because `appsscript.json` had `oauthScopes` configured (for bound script) but no `webapp` section. The webapp needed to be accessible without authentication for n8n integration.

**What changed:**
- `appsscript.json`: Replaced `oauthScopes` array with `webapp` configuration:
  ```json
  "webapp": {
    "access": "ANYONE_ANONYMOUS",
    "executeAs": "USER_DEPLOYING"
  }
  ```
- This allows anyone with the deployment URL to access the Web App endpoints
- OAuth scopes are inferred automatically by GAS for bound scripts

### 2026-02-09: OAuth Setup for Google Sheets/Drive API

**Purpose:** Enable Claude Code to directly read/write to Google Sheets and Drive for E2E testing and debugging.

**What was set up:**
- Created OAuth 2.0 credentials in Google Cloud Console (project: video-analytics-hub)
- Created `scripts/gsheet.py` CLI utility for direct Sheets/Drive API access
- Token stored in `.gsheets_token.json` (gitignored)
- OAuth client credentials in `video_analytics_hub_claude_code_oauth.json` (gitignored)
- MCP servers configured in `.mcp.json` for google-workspace and google-sheets integration (gitignored)

### 2026-02-09: E2E Testing Results

**End-to-end testing was performed against the live spreadsheet:**
- Confirmed all 9 sheet tabs exist with correct headers
- Confirmed master sheet has 3 demo videos (2 analyzed, 1 draft)
- Confirmed metrics sheets have data for analyzed videos
- Confirmed KPI targets are set with default values
- Confirmed component inventories are connected and populated with demo data
- All 330 unit tests pass

### 2026-02-09: LLM Analysis Execution (Live Test)

**Live test of the analysis pipeline was verified:**
- OpenAI API key configured via `_config` sheet (auto-migrated to Script Properties)
- `analyzeVideoSingle` and `analyzeAllVideosEnhanced` functions tested via GAS editor execution
- Analysis writes to `video_analysis` and `recommendations` sheets
- Component scores updated after analysis
- AI-recommended components written to `ai_next_*` columns

### 2026-02-09: MANUAL.md Creation

**Created comprehensive Japanese-language user manual (`MANUAL.md`):**
- Complete system overview with architecture diagrams (ASCII art)
- Detailed Google Drive data structure documentation
- All spreadsheet schemas with column descriptions
- Component management workflows
- Video production lifecycle documentation
- CSV import procedures for all 3 platforms
- Analysis and KPI explanation
- AI recommendation and approval flow
- n8n/API integration guide with all endpoint specifications
- GAS menu operation guide
- Troubleshooting section
- Initial setup instructions

### 2026-02-09: v2.1.0 - Documentation Update and .gitignore Hardening

**What was done:**
- Updated CONTEXT.md with complete project history
- Added `.mcp.json` and `tmp/` to `.gitignore` to prevent credential leaks
- Verified all documentation files are consistent with v2.0 codebase
- Committed, pushed, and deployed to Apps Script

### 2026-02-09: v3.0.0 - AI-Influencer Pipeline

**Why:** Pivot from analytics-only tool to a full content production pipeline. The existing GAS analytics (v2.0) handles post-publication analysis well, but the upstream workflow (video generation → platform posting) was manual via n8n. This version adds an automated Node.js pipeline to handle end-to-end content production at scale (50 → 700 accounts).

**What was done:**

- **Project restructure**: Renamed conceptually from "Video Analytics Hub" to "AI-Influencer" to reflect the broader scope
- **Created `pipeline/` directory** with modular Node.js architecture:
  - `pipeline/config.js` - Environment configuration and API key management
  - `pipeline/orchestrator.js` - Pipeline orchestration (scenario → video → post)
  - `pipeline/sheets/reader.js` - Google Sheets API reader (scenarios, accounts)
  - `pipeline/sheets/writer.js` - Google Sheets API writer (pipeline status, results)
  - `pipeline/media/kling.js` - fal.ai Kling video generation
  - `pipeline/media/tts.js` - fal.ai ElevenLabs TTS
  - `pipeline/media/lipsync.js` - fal.ai Lipsync processing
  - `pipeline/media/creatify.js` - Creatify video composition
  - `pipeline/storage/drive.js` - Google Drive upload/management
  - `pipeline/posting/youtube.js` - YouTube Data API v3 posting
  - `pipeline/posting/instagram.js` - Instagram Graph API posting
  - `pipeline/posting/tiktok.js` - TikTok Content Posting API
  - `pipeline/posting/x.js` - X/Twitter v2 API posting
- **Created `scripts/` entry points**:
  - `scripts/run-pipeline.js` - Single video pipeline execution
  - `scripts/run-daily.js` - Daily batch execution for all accounts
  - `scripts/collect-metrics.js` - Metrics collection from platforms
- **New Google Sheets tabs designed**:
  - `accounts` - Platform account management (account_id, platform, credentials_ref, status, daily limits)
  - `content_pipeline` - Pipeline execution log (pipeline_id, video_uid, account_id, job IDs, status, cost)
- **Documentation rewrite**:
  - `STRATEGY.md` (NEW) - Strategy, KPI targets, revenue model, meeting notes, decision log
  - `README.md` - Complete rewrite for AI-Influencer scope
  - `ARCHITECTURE.md` - Complete rewrite with Pipeline + Analytics architecture
  - `CONTEXT.md` - Appended this session

**Key decisions:**
1. **Node.js pipeline (not GAS)**: GAS has 6-minute timeout and can't handle long-running video generation. Node.js runs on local/server with no timeout constraints
2. **fal.ai as media hub**: Consolidates Kling, ElevenLabs, and Lipsync under one API provider
3. **Google Sheets as DB**: Continues using Sheets for simplicity and compatibility with existing GAS analytics
4. **YouTube first for MVP**: Most stable API, fastest path to revenue
5. **Existing GAS analytics unchanged**: The 14 GAS files and 330 tests remain untouched

**Phase plan:**
- Phase 1 (2/10-12): MVP - 1 account, YouTube auto-posting
- Phase 2 (2/13-17): 12 YouTube + 12 Instagram accounts
- Phase 3 (2/18-28): All 50 accounts across 4 platforms

### 2026-02-10: Phase 0 - Content Generation Pipeline Implementation

**Why:** The n8n workflow analysis revealed 5 critical differences between the existing pipeline code and the actual production workflow. The pipeline needed to be rewritten to match the real n8n flow exactly.

**What was done:**

- **n8n workflow analysis**: Analyzed `ワークフロー.json` and `senario_template.xlsx` to extract the real 3-step process (Kling → TTS → Lipsync, no Creatify Aurora)
- **Created `pipeline/data/scenario.json`**: Extracted 3 sections (hook, body, cta) with scripts and motion reference video Drive IDs (`motionVideoDriveId`)
- **Fixed media module endpoints**:
  - `video-generator.js`: `image-to-video` → `motion-control` with `video_url`, `character_orientation` (NO `prompt` or `keep_original_sound` — causes 422)
  - `tts-generator.js`: `tts/v3` → `tts/eleven-v3` with voice name "Aria" instead of voice_id
  - `lipsync.js`: `v2` → `v2/pro` with `sync_mode: "bounce"`
- **Added fal.storage upload**: `uploadToFalStorage()` and `downloadFromDrive()` in `fal-client.js` — replaces Cloudinary with Drive → fal.storage → API pattern
- **Created ffmpeg concat utility**: `pipeline/media/concat.js` using concat demuxer for combining 3 section videos
- **Rewrote orchestrator**: New 3-section loop — downloads motion video from Drive per section → fal.storage → Kling → TTS → Lipsync, then ffmpeg concat + Drive upload (4 files) + sheet logging
- **Rewrote CLI**: `--character-folder <DRIVE_FOLDER_ID>` replaces `--account`
- **Deleted Cloudinary and Compositor modules**: Removed `compositor.js`, `cloudinary.js`, cloudinary config and dependency
- **Updated content_pipeline sheet schema**: New columns (character_folder_id, section_count, hook/body/cta_video_url, drive_folder_id)
- **Updated content-manager.js**: HEADERS array matches new sheet schema
- **Created pipeline test suite**: 21 tests covering modules, endpoints, ffmpeg, CLI, schema, edge cases
- **Updated Jest config**: Projects-based config for both GAS and pipeline test suites
- **E2E success**: First full pipeline run completed — CNT_202602_2916 (54MB final video, 3 sections)
  - Uploaded CHR_0001_v1.jpg to `Characters/Images/CHR_0001/`
  - Fixed `FAL_KEY || FAL_AI_KEY` fallback in config.js
  - Replaced dead Cloudinary URLs in scenario.json with Drive file IDs (motionVideoDriveId)
  - Removed invalid `prompt: ''` and `keep_original_sound: true` params from video-generator
  - Output: `Productions/2026-02-10/CNT_202602_2916/` with 4 MP4 files uploaded to Drive
  - content_pipeline sheet updated with status=completed and all Drive links
- **README.md rewritten**: Comprehensive specification document with pipeline flow, service descriptions, Drive folder structure, all Sheets schemas with column-level documentation, setup, usage, cost structure

**Key decisions:**
1. **Creatify Aurora removed**: n8n workflow doesn't use it — the 3-step process (Kling → TTS → Lipsync) is sufficient
2. **Cloudinary → fal.storage**: All persistent files in Drive, temporary URLs via fal.storage. No external image hosting needed
3. **3-section structure**: Each video consists of hook + body + cta, processed independently then concatenated
4. **ffmpeg for concatenation**: Simple `concat demuxer` with `-c copy` (no re-encoding) for fast joining
5. **Drive folder structure**: Productions/YYYY-MM-DD/CNT_XXXX/ with 4 files per content
6. **Motion videos from Drive**: scenario.json stores Drive file IDs (not URLs), orchestrator downloads + uploads to fal.storage per section

**Test results:** 357 tests passing (330 GAS + 27 pipeline)

### 2026-02-10: v4.0 - Production Rebuild

**Why:** Phase 0 proved the E2E pipeline works, but it relied on demo data and sequential processing. To hit the 36 videos/day target for 50 accounts by February, the system needed: (1) real data in all inventories, (2) parallel processing for speed, (3) proper inventory management modules, and (4) a production-grade tracking tab.

**What was done:**

- **Demo data → real data migration**:
  - Cleared all demo data from 4 inventory spreadsheets (Scenarios, Motions, Characters, Audio)
  - Populated inventories with real data from テンプレ.xlsx
  - Uploaded 3 motion videos to Drive (Hooks/Bodies/CTAs folders)

- **Accounts Inventory** (new spreadsheet):
  - 7 platform accounts configured
  - 12 Gmail credentials registered

- **Production tab** (new, 32 columns):
  - Replaces content_pipeline for new pipeline runs
  - Tracks video_id, all inventory IDs used, scripts, output URLs, processing time, cost, error details
  - content_pipeline tab preserved for backward compatibility (v3.1 records remain)

- **New pipeline modules**:
  - `pipeline/sheets/inventory-reader.js`: Reads from all 4 inventory spreadsheets, resolves component IDs to Drive file IDs and metadata
  - `pipeline/sheets/production-manager.js`: Creates/updates rows in the production tab, manages status transitions

- **Orchestrator parallelization** (`pipeline/orchestrator.js` rewrite):
  - 3 sections now processed in parallel via `Promise.all`
  - Within each section, Kling video generation and TTS run in parallel
  - Only Lipsync runs sequentially (requires both video and audio as input)
  - Processing time reduced from ~35min to ~12min per video

- **CLI update** (`scripts/run-pipeline.js`):
  - New flags: `--video-id <ID>`, `--limit <N>`
  - `--character-folder` deprecated (still works for backward compatibility)
  - Pipeline reads from inventory sheets instead of scenario.json

- **Deprecations**:
  - `pipeline/data/scenario.json`: No longer the primary data source; inventories are used instead
  - `--character-folder` CLI flag: Replaced by `--video-id`

- **Config updates** (`pipeline/config.js`):
  - Added `ACCOUNTS_SPREADSHEET_ID` for Accounts Inventory
  - Added `productionTab` configuration

**Key decisions:**
1. **Parallel Promise.all for 3 sections**: Each section is independent until lipsync, so process all 3 simultaneously
2. **Kling + TTS parallel within sections**: Video generation and audio generation don't depend on each other
3. **Production tab replaces content_pipeline for new runs**: Cleaner schema with 32 columns, keeps legacy data intact
4. **Inventory-driven pipeline**: scenario.json was static and limiting; reading from Sheets allows dynamic content selection
5. **Backward compatibility maintained**: `--character-folder` still works, content_pipeline tab preserved

**Files changed/created:**
- New: `pipeline/sheets/inventory-reader.js`, `pipeline/sheets/production-manager.js`
- Modified: `pipeline/orchestrator.js` (parallel rewrite), `pipeline/config.js` (accounts ID, productionTab), `scripts/run-pipeline.js` (new CLI flags)
- Deprecated: `pipeline/data/scenario.json`
- Docs: `STRATEGY.md`, `ARCHITECTURE.md`, `README.md`, `CONTEXT.md` all updated

**New Resource IDs:**
| Resource | ID |
|----------|-----|
| Accounts Inventory | `1CmT6C3qCW3md6lJ9Rvc2WNQkWa5zcvlq6Zp_enJHoUE` |

**Test results:** 357 tests passing (330 GAS + 27 pipeline)

**Bug fixes (same session):**
- `drive_file_id` column missing from Motions/Characters inventories → added to both
- `extractDriveFileId()` helper added to orchestrator.js: defensive fallback that extracts file ID from `drive_file_id` field or parses it from `file_link` URL patterns
- `fal-client.js`: enriched error messages now include fal.ai body details (e.g., balance exhausted reason)

**E2E real API result (v4.0 parallel pipeline — SUCCESS):**
- VID_202602_0001: 3 sections parallel, 1030s (17.2min) total
- Hook: 9.2MB, Body: 78.4MB, CTA: 17.3MB, Final: 104.9MB (100MB)
- Drive folder: `1IiMhXW7WfyCWADSj-aNF_-AWuzuv-pyy`
- Production tab auto-updated: pipeline_status=completed, all 4 video URLs, processing_time=1030s
- All API steps succeeded: character upload → 3× (motion upload + Kling + TTS parallel + Lipsync) → ffmpeg → Drive

**New documentation:**
- `OPERATIONS.md`: 667-line Japanese operations manual for human operators
  - Covers: inventory management, production workflow, CLI usage, troubleshooting, cost reference

### 2026-02-11: Documentation Audit & Reorganization

**Why:** ドキュメントが根幹レベルに散乱し、技術情報とビジネス情報が混在し、同じ情報が複数ファイルに重複していた。50→3,500アカウントのスケーリングに備え、チームメイトが迷わず参照できる体制を整備。

**What was done:**

- **ファイル構造の再編成**:
  - `STRATEGY.md` → `docs/STRATEGY.md`
  - `ARCHITECTURE.md` → `docs/ARCHITECTURE.md`
  - `OPERATIONS.md` → `docs/manuals/OPERATIONS.md`
  - `MANUAL.md` → `docs/manuals/GAS_MANUAL.md`
  - `docs/USER_GUIDE.md` → `docs/manuals/USER_GUIDE.md`
  - `docs/account-design-guide.md` → `docs/manuals/account-design-guide.md`
  - ルートには `README.md` と `CONTEXT.md` のみ残す

- **ドキュメントポリシー確立**:
  - **README.md**: 技術SSOT（スキーマ、コスト、Drive構造の唯一の定義場所）
  - **docs/STRATEGY.md**: ビジネス専用（KPI、収益、フェーズ、リスク、意思決定）— 技術的な仕様は含めない
  - **docs/ARCHITECTURE.md**: システム全体図、データフロー図、モジュール一覧
  - **docs/manuals/**: 人間オペレーター向けガイド
  - **重複禁止**: 各情報は1箇所のみに記載し、他はクロスリファレンス

- **重複コンテンツの除去**:
  - ARCHITECTURE.md: Sheetsスキーマ（110行）、コストセクション（20行）を削除 → README参照に変更
  - OPERATIONS.md: コスト参考（27行）、スプレッドシートスキーマ一覧（82行）を削除 → README参照に変更

- **KPI数値更新**（マスターシート.xlsxから）:
  - 5月: 520 → **1,480** アカウント
  - 6月: 700 → **3,500** アカウント

- **クロスリファレンス修正**: 全ファイルのマークダウンリンクを新パスに更新

- **account-design-guide.md**: チームメイト向けアカウント設計ガイド作成（ペルソナ設計、画像準備、シナリオ作成、インベントリ登録手順、テンプレート、FAQ）

- **セキュリティ監査**: git履歴・全ファイルをスキャンし、シークレット漏洩がないことを確認

**Commits:**
- `85ae670` Docs: audit cleanup — add docs index, fix stale refs, remove secrets
- `940ca10` Docs: add account design guide + tech stack table
- `6f4db09` Docs: reorganize structure, deduplicate content, update KPI numbers

### 2026-02-11: コスト分析 & システム設計ドキュメント

**Why:** コスト計算が10秒ベースで算出されていたが、実際のパイプライン設定は5秒/セクション。正確な単価に修正し、ステークホルダー向けの詳細コスト分析レポートを作成。また、50→3,500アカウントスケーリングに向けた全体システム設計と SNS API 調査が必要だった。

**What was done:**

- **コスト修正** ($3.72/本 → $2.34/本):
  - Kling: $0.70 → $0.35（5秒×$0.07/秒）
  - ElevenLabs: ~$0.04 → ~$0.01
  - Lipsync: $0.50 → $0.42（5秒/60×$5.00/分）
  - README.md, ARCHITECTURE.md, OPERATIONS.md を全て更新

- **新規ドキュメント**:
  - `docs/cost-analysis/per-video-cost.md`: 1動画あたりのAPI費用内訳
  - `docs/cost-analysis/per-minute-cost.md`: 分単価比較（API vs GUI、他社サービス比較）
  - `docs/cost-analysis/full-system-design.md`: 全9フェーズのシステム設計書（614行）
    - 市場リサーチ → アカウント設計 → コンテンツ作成 → 投稿 → 計測 → 分析 → 改善のフルループ
    - 月別コスト試算（2月〜6月）、Pro/Std 2プラン比較
    - インフラ構成、並列化戦略、レート制限分析
  - `docs/research/SNS_API_RESEARCH.md`: 4プラットフォームのAPI調査レポート（615行）
    - YouTube / TikTok / Instagram / X の投稿API仕様、制限、認証方式

- **STRATEGY.md更新**: システム設計書・SNS研究へのクロスリファレンス追加

**Commits:**
- `9ef6f94` Docs: add cost analysis reports, fix API pricing across all docs
- `0e3f3eb` Docs: add full system design & SNS API research for 50→3,500 account scaling

---

## Current System State (as of 2026-02-11)

### Deployed and Working (GAS Analytics v2.0)
- Master Spreadsheet with all 9 tabs initialized and formatted
- 4 Component Inventory spreadsheets (Scenarios, Motions, Characters, Audio)
- Google Drive folder structure (AI-Influencer root with all subfolders)
- Web App deployed with anonymous access
- OpenAI integration working (via _config sheet fallback)
- Demo data: 3 videos (2 analyzed with metrics, 1 draft)
- Demo components: 7 scenarios, 5 motions, 3 characters, 4 audio
- All 330 GAS tests passing

### Implemented (Pipeline v4.0 - 2026-02-10) — Production Rebuild
- All inventory spreadsheets populated with real data (テンプレ.xlsx)
- 3 motion videos uploaded to Drive (Hooks/Bodies/CTAs)
- Accounts Inventory: 7 accounts + 12 Gmail credentials
- Production tab: 32-column tracking in Master Spreadsheet
- `inventory-reader.js`: Reads inventories, resolves component IDs
- `production-manager.js`: Manages production tab CRUD
- Orchestrator parallelized: 3 sections via Promise.all, Kling+TTS parallel within sections
- Processing time: ~35min → ~12min per video
- CLI: `node scripts/run-pipeline.js --video-id <ID> [--limit N] [--dry-run]`
- `--character-folder` deprecated (backward compatible)
- `scenario.json` deprecated (inventories used instead)
- 26 pipeline tests passing

### Implemented (Pipeline Phase 0 - 2026-02-10) — E2E Verified
- Content generation pipeline: character image → 3-section video → Drive — **working end-to-end**
- First successful E2E: CNT_202602_2916 (54MB final video, ~35min processing time)
- 3 media modules (Kling motion-control, ElevenLabs eleven-v3, Lipsync v2/pro)
- fal.storage upload (replaces Cloudinary)
- ffmpeg concat utility for 3-section video joining
- content_pipeline sheet with schema (15 columns) — 3 records (2 error during dev, 1 completed)
- Character image CHR_0001_v1.jpg uploaded to Drive `Characters/Images/CHR_0001/`
- Output stored at `Productions/2026-02-10/CNT_202602_2916/` (4 MP4 files)

### Documentation (2026-02-11 reorganized)
- ドキュメントポリシー確立: README=技術SSOT, STRATEGY=ビジネス専用, 重複禁止
- コスト計算修正: $3.72 → $2.34/本（5秒/セクション）
- 全9フェーズシステム設計書 + SNS API調査レポート作成済み
- account-design-guide: チームメイト向けアカウント設計ガイド作成済み

### Pending
- YouTube OAuth セットアップ（最初のアカウント向け）
- Platform posting automation（YouTube → Instagram → TikTok → X）
- Batch execution with `--limit` for multiple videos
- Metrics collection automation
- GAS analytics integration with production tab data
- ClipPulse integration for market research (under consideration)
- OpenClaw integration for operation automation (under consideration)

### Files in Repository
```
gas/                            # GAS analytics (unchanged since v2.0)
  Code.gs                       # Web App endpoints + UI menu (1157 lines)
  Config.gs                     # Settings, schema, constants (389 lines)
  Setup.gs                      # One-click system setup (762 lines)
  Migration.gs                  # v1 to v2 migration (224 lines)
  CSVParser.gs                  # Platform-specific CSV parsers (190 lines)
  Normalizer.gs                 # Unified schema conversion (208 lines)
  Linker.gs                     # video_uid matching (238 lines)
  KPIEngine.gs                  # KPI comparison (249 lines)
  LLMAnalyzer.gs                # OpenAI integration (665 lines)
  SheetWriter.gs                # Sheet write operations (275 lines)
  ComponentManager.gs           # Component CRUD + context (283 lines)
  MasterManager.gs              # Master sheet + workflow (255 lines)
  ScoreUpdater.gs               # Component scoring (212 lines)
  Utils.gs                      # ID generators, helpers (544 lines)
  appsscript.json               # GAS manifest
  tests/                        # 9 test suites, 330 tests

pipeline/                       # Node.js content pipeline (v4.0)
  config.js                     # Environment configuration
  orchestrator.js               # 3-section parallel pipeline (Promise.all)
  data/scenario.json            # Scenario template (DEPRECATED)
  sheets/client.js              # Google Sheets/Drive API (OAuth2)
  sheets/inventory-reader.js    # Inventory reader + ID resolution (v4.0)
  sheets/production-manager.js  # Production tab CRUD (v4.0)
  sheets/account-manager.js     # Accounts tab CRUD
  sheets/content-manager.js     # Content pipeline tab CRUD (legacy)
  media/fal-client.js           # fal.ai SDK + fal.storage upload + Drive download
  media/video-generator.js      # Kling v2.6 motion-control
  media/tts-generator.js        # ElevenLabs eleven-v3 (voice: Aria)
  media/lipsync.js              # Sync Lipsync v2/pro (sync_mode: bounce)
  media/concat.js               # ffmpeg concat demuxer
  storage/drive-storage.js      # Video URL → Drive upload
  posting/poster.js             # Unified posting interface (stub)
  posting/adapters/*.js         # Platform adapters (stubs)

tests/                          # Pipeline tests
  pipeline.test.js              # 27 tests

scripts/                        # CLI entry points
  run-pipeline.js               # --video-id <ID> [--limit N] [--dry-run]
  run-daily.js                  # Daily batch execution (stub)
  collect-metrics.js            # Metrics collection (stub)
  gsheet.py                     # Google Sheets CLI utility

docs/                           # ドキュメント（2026-02-11 再編成済み）
  STRATEGY.md                   # ビジネス戦略（KPI, 収益, フェーズ, 意思決定）
  ARCHITECTURE.md               # 技術アーキテクチャ（システム図, データフロー）
  n8n-integration.md            # n8n ワークフロー連携ガイド
  cost-analysis/
    per-video-cost.md           # 1動画あたりAPI費用内訳
    per-minute-cost.md          # 分単価比較（API vs GUI）
    full-system-design.md       # 全9フェーズ システム設計書
  research/
    SNS_API_RESEARCH.md         # SNS API 調査レポート（YT/TT/IG/X）
  manuals/
    OPERATIONS.md               # 運用マニュアル（日本語）
    GAS_MANUAL.md               # GAS操作マニュアル（日本語）
    USER_GUIDE.md               # ユーザーガイド（日本語）
    account-design-guide.md     # アカウント設計ガイド（チーム向け）

README.md                       # 技術SSOT（スキーマ, コスト, Drive構造）
CONTEXT.md                      # プロジェクト履歴（このファイル）
```

---

## Technical Notes

### Platform CSV Column Variations

YouTube may change column names between exports. Known variations:
- "Views" / "View count" / "視聴回数"
- "Watch time (hours)" / "総再生時間（時間）"

TikTok variations:
- "Video views" / "Views"
- "Average watch time" / "Avg. watch time"

Instagram variations:
- "Plays" / "Views"
- "Reach" / "Accounts reached"

### GAS Limitations to Remember
- 6-minute execution timeout
- 20MB response size limit
- 100 triggers per user limit
- Properties Service: 500KB total, 9KB per property

### OpenAI Integration Notes
- Use TSV output format for structured responses (more reliable parsing than JSON)
- Batch multiple videos per request to reduce API calls
- Implement exponential backoff for rate limiting
- v2.0: Include component context for better recommendations

### v2.0 Component Score System
- Each analyzed video's overall_score contributes to its components' avg_performance_score
- Scores normalized 0-100 across platforms
- Top-performing components surfaced in AI recommendation prompts
- Score updates cascade: video analyzed -> master updated -> all linked components updated

### Sensitive Data Locations (NOT in git)
- `.clasp.json` - clasp config with Script ID
- `.gsheets_token.json` - OAuth token for Sheets/Drive API
- `video_analytics_hub_*oauth*.json` - OAuth client credentials
- `client_secret_*.json` - Google Cloud client secrets
- `.mcp.json` - MCP server config with OAuth credentials
- `_config` sheet in spreadsheet - contains OpenAI API key
